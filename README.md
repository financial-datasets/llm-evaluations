# LLM Evaluations

A practical repository for evaluating Large Language Models (LLMs) on various financial research tasks. This project provides tools and methodologies to assess LLM performance, accuracy, and reliability in financial analysis and research contexts.

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/financial-datasets/llm-evaluations.git
   cd llm-evaluations
   ```

2. **Install dependencies**
   ```bash
   uv sync
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys and configuration
   ```

4. **Run the example**
   ```bash
   uv run main.py
   ```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

[Add your license information here]
