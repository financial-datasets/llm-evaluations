# LLM Evaluations

A codebase for evaluating Large Language Models (LLMs) on financial research tasks. Contains tools and methodologies to assess LLM performance, accuracy, and reliability in financial analysis. 

[![Twitter Follow](https://img.shields.io/twitter/follow/findatasets?style=social)](https://twitter.com/findatasets)

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Contributing](#-contributing)
- [License](#-license)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) package manager

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/financial-datasets/llm-evaluations.git
   cd llm-evaluations
   ```

2. **Install dependencies**
   ```bash
   uv sync
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys and configuration
   ```

4. **Run the example**
   ```bash
   uv run main.py
   ```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License